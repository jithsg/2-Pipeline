{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHRbjj5MeGh7uqGa0DoS7B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jithsg/2-Pipeline/blob/main/taxonomy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU2zYoB_hPoC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import zipfile\n",
        "import io\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "import os\n",
        "import shutil\n",
        "import math\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 1. Configuration and Constants\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Define the taxonomy structure (required for coverage and entropy calculations)\n",
        "TAXONOMY_DEFS = {\n",
        "    'M': ['VAE', 'GAN', 'FLOW', 'DIFF', 'AUTOREG/LLM', 'G-GEN', 'EBM/CONTR', 'HYB/RAG'],\n",
        "    'A': ['DAUG', 'ADSIM', 'DEF', 'REP/UNSUP', 'PRIV', 'FED-GEN', 'MULTI-MOD', 'TINT', 'XAI/CFEX', 'FORE', 'BENCH', 'ADAPT-IDS'],\n",
        "    'E': ['CLOUD', 'EDGE/IoT', '5G/6G', 'ICS/SCADA/OT', 'HYBRID'],\n",
        "    'D': ['NET', 'LOG', 'HOST', 'GRAPH', 'BIN', 'MM'],\n",
        "    'L': ['SUP', 'SEMI', 'UNSUP', 'SELF/CONTR', 'FED', 'CONT', 'FS'],\n",
        "    'T': ['TRAIN', 'INFER', 'RB-SIM', 'MDR', 'ADAPT']\n",
        "}\n",
        "AXES = list(TAXONOMY_DEFS.keys())\n",
        "OUTPUT_BASE_DIR = Path(\"taxonomy_validation_reproduced\")\n",
        "OUTPUT_DIR = OUTPUT_BASE_DIR / \"outputs\"\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 2. Data Loading (Using Embedded Coder A and B Data)\n",
        "# ----------------------------------------------------------------------\n",
        "# This block combines the Coder A and Coder B data provided previously\n",
        "# to create the input dataframe for the analysis.\n",
        "\n",
        "def load_coder_data():\n",
        "    \"\"\"Loads the Coder A and B data from the previous session.\"\"\"\n",
        "\n",
        "    # Coder A Data (As provided in the initial analysis)\n",
        "    CODER_A_DATA = \"\"\"paper_id,M,A,E,D,L,T\n",
        "Aidin_Ferdowsi.txt,GAN,FED-GEN,EDGE/IoT,NET,FED,INFER\n",
        "Akim_Kotelnikov.txt,DIFF,PRIV,CLOUD,NET,UNSUP,TRAIN\n",
        "Enyan_Dai.txt,FLOW,REP/UNSUP,CLOUD,GRAPH,UNSUP,INFER\n",
        "Eunbi_Seo.txt,GAN,REP/UNSUP,EDGE/IoT,NET,UNSUP,INFER\n",
        "Hang_Shen.txt,HYB/RAG,DAUG,CLOUD,NET,SUP,TRAIN\n",
        "Mohamed_Amine_Merzouk.txt,DIFF,DEF,CLOUD,NET,UNSUP,INFER\n",
        "Mohammad_Jamoos.txt,GAN,DAUG,CLOUD,NET,SUP,TRAIN\n",
        "Nour_Alhussien.txt,DIFF,DEF,HYBRID,NET,UNSUP,ADAPT\n",
        "Pardis_Sadatian_Moghaddam.txt,HYB/RAG,MULTI-MOD,EDGE/IoT,NET,SUP,TRAIN\n",
        "Sahar_Aldhaheri.txt,GAN,TINT,CLOUD,NET,SELF/CONTR,RB-SIM\n",
        "Sultan_Zavrak.txt,VAE,REP/UNSUP,HYBRID,NET,SEMI,INFER\n",
        "Vikash_Kumar.txt,GAN,DAUG,CLOUD,NET,SUP,TRAIN\n",
        "Xiang_LUO.txt,VAE,ADAPT-IDS,CLOUD,NET,SEMI,INFER\n",
        "Xiao_Han.txt,AUTOREG/LLM,REP/UNSUP,HYBRID,LOG,SELF/CONTR,INFER\n",
        "Yajun_Chen.txt,GAN,DAUG,EDGE/IoT,NET,SUP,TRAIN\n",
        "Yang_Yang.txt,GAN,DAUG,CLOUD,MM,SUP,TRAIN\n",
        "Yonas_Teweldemedhin_Gebrezgiher.txt,VAE,REP/UNSUP,EDGE/IoT,NET,UNSUP,INFER\n",
        "Yue_Yang.txt,DIFF,DAUG,CLOUD,NET,SUP,TRAIN\n",
        "Zhengfa_Li.txt,HYB/RAG,DAUG,CLOUD,NET,SUP,TRAIN\n",
        "Zijie_Chen.txt,DIFF,DAUG,CLOUD,NET,SUP,TRAIN\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Coder B Data (As provided in the initial analysis)\n",
        "    CODER_B_DATA = \"\"\"paper_id,M,A,E,D,L,T\n",
        "Aidin_Ferdowsi.txt,GAN,FED-GEN,EDGE/IoT,NET,FED,INFER\n",
        "Akim_Kotelnikov.txt,DIFF,DAUG,CLOUD,NET,UNSUP,TRAIN\n",
        "Enyan_Dai.txt,FLOW,REP/UNSUP,CLOUD,MM,UNSUP,INFER\n",
        "Eunbi_Seo.txt,GAN,REP/UNSUP,EDGE/IoT,NET,UNSUP,INFER\n",
        "Hang_Shen.txt,GAN,DAUG,CLOUD,NET,SUP,TRAIN\n",
        "Mohamed_Amine_Merzouk.txt,DIFF,ADSIM,CLOUD,NET,UNSUP,TRAIN\n",
        "Mohammad_Jamoos.txt,GAN,DAUG,CLOUD,NET,SUP,TRAIN\n",
        "Nour_Alhussien.txt,DIFF,ADSIM,CLOUD,NET,UNSUP,INFER\n",
        "Pardis_Sadatian_Moghaddam.txt,HYB/RAG,MULTI-MOD,EDGE/IoT,NET,SUP,TRAIN\n",
        "Sahar_Aldhaheri.txt,GAN,TINT,CLOUD,NET,SUP,RB-SIM\n",
        "Sultan_Zavrak.txt,VAE,REP/UNSUP,HYBRID,NET,SEMI,INFER\n",
        "Vikash_Kumar.txt,GAN,DAUG,CLOUD,NET,SUP,TRAIN\n",
        "Xiang_LUO.txt,VAE,ADAPT-IDS,CLOUD,NET,SEMI,INFER\n",
        "Xiao_Han.txt,AUTOREG/LLM,REP/UNSUP,CLOUD,LOG,SELF/CONTR,INFER\n",
        "Yajun_Chen.txt,GAN,DAUG,EDGE/IoT,NET,SUP,TRAIN\n",
        "Yang_Yang.txt,GAN,DAUG,CLOUD,MM,SUP,TRAIN\n",
        "Yonas_Teweldemedhin_Gebrezgiher.txt,VAE,REP/UNSUP,5G/6G,NET,UNSUP,INFER\n",
        "Yue_Yang.txt,DIFF,DAUG,CLOUD,NET,SUP,TRAIN\n",
        "Zhengfa_Li.txt,HYB/RAG,DAUG,CLOUD,NET,SUP,TRAIN\n",
        "Zijie_Chen.txt,DIFF,DAUG,CLOUD,NET,SUP,TRAIN\"\"\"\n",
        "\n",
        "#Akim_Kotelnikov->NET\n",
        "#Yonas_Teweldemedhin_Gebrezgiher: NET\n",
        "\n",
        "    df_a = pd.read_csv(io.StringIO(CODER_A_DATA))\n",
        "    df_a['coder'] = 'A'\n",
        "    df_b = pd.read_csv(io.StringIO(CODER_B_DATA))\n",
        "    df_b['coder'] = 'B'\n",
        "    df_combined = pd.concat([df_a, df_b], ignore_index=True)\n",
        "\n",
        "    # Add placeholder metadata columns for completeness\n",
        "    for col in ['title', 'year', 'venue', 'doi']:\n",
        "        df_combined[col] = 'N/A'\n",
        "\n",
        "    # Reorder columns\n",
        "    cols_order = ['paper_id', 'title', 'year', 'venue', 'doi', 'coder'] + AXES\n",
        "    df_combined = df_combined[cols_order]\n",
        "\n",
        "    return df_combined\n",
        "\n",
        "# Setup environment: Clean previous runs and create directories\n",
        "if OUTPUT_BASE_DIR.exists():\n",
        "    try:\n",
        "        if os.path.isdir(OUTPUT_BASE_DIR):\n",
        "            shutil.rmtree(OUTPUT_BASE_DIR)\n",
        "    except OSError as e:\n",
        "        print(f\"Warning: Could not remove existing directory {OUTPUT_BASE_DIR}: {e}\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load the data\n",
        "print(\"Loading Coder A and B data...\")\n",
        "df = load_coder_data()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 3. Reconciliation and IRR Calculation\n",
        "# ----------------------------------------------------------------------\n",
        "# This step compares Coder A and B to calculate Kappa and determines the final labels.\n",
        "\n",
        "def reconcile_and_calculate_irr(df):\n",
        "    \"\"\"\n",
        "    Reconciles labels between Coder A and B, and calculates Cohen's Kappa.\n",
        "    \"\"\"\n",
        "    # Ensure no duplicate entries for the same paper_id and coder before pivoting\n",
        "    df_deduped = df.drop_duplicates(subset=['paper_id', 'coder'])\n",
        "\n",
        "    # Pivot the data so Coder A and Coder B labels are side-by-side for comparison\n",
        "    df_pivot = df_deduped.pivot(index='paper_id', columns='coder', values=AXES)\n",
        "\n",
        "    kappa_scores = {}\n",
        "    df_reconciled = pd.DataFrame(index=df_pivot.index)\n",
        "\n",
        "    for axis in AXES:\n",
        "        # Get the labels from both coders\n",
        "        # Using 'NaN_STR' to handle potential missing values consistently during comparison\n",
        "        codes_A = df_pivot[(axis, 'A')].fillna('NaN_STR').astype(str)\n",
        "        codes_B = df_pivot[(axis, 'B')].fillna('NaN_STR').astype(str)\n",
        "\n",
        "        # Calculate Kappa (Inter-Rater Reliability)\n",
        "        try:\n",
        "            kappa = cohen_kappa_score(codes_A, codes_B)\n",
        "            kappa_scores[axis] = kappa\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not calculate Kappa for axis {axis}: {e}\")\n",
        "            kappa_scores[axis] = np.nan\n",
        "\n",
        "        # Reconcile: If they agree, use the agreed label. If they disagree, prefer Coder A.\n",
        "        # In this implementation strategy, the reconciled result defaults to Coder A's labels.\n",
        "        df_reconciled[axis] = codes_A\n",
        "\n",
        "    # Calculate Macro Kappa (Average Kappa across all axes)\n",
        "    valid_kappas = [k for k in kappa_scores.values() if not np.isnan(k)]\n",
        "    macro_kappa = np.mean(valid_kappas) if valid_kappas else np.nan\n",
        "\n",
        "    # Replace 'NaN_STR' strings back to actual NaN for subsequent analysis\n",
        "    df_reconciled = df_reconciled.replace('NaN_STR', np.nan)\n",
        "\n",
        "    return df_reconciled, kappa_scores, macro_kappa\n",
        "\n",
        "print(\"Calculating IRR and reconciling data...\")\n",
        "df_reconciled, kappa_scores, macro_kappa = reconcile_and_calculate_irr(df)\n",
        "N_papers = len(df_reconciled)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 4. Metrics Calculation (Coverage, Entropy, Under-explored Pairs)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Coverage % (share of papers with valid, non-NA labels on all six axes)\n",
        "valid_papers = df_reconciled.dropna(subset=AXES).shape[0]\n",
        "overall_data_coverage = (valid_papers / N_papers) * 100 if N_papers > 0 else 0\n",
        "\n",
        "def calculate_coverage(df_reconciled):\n",
        "    \"\"\"Calculates the percentage of defined categories utilized for each axis.\"\"\"\n",
        "    coverage_metrics = {}\n",
        "\n",
        "    # Calculate coverage (utilization) of the taxonomy definitions\n",
        "    for axis, categories in TAXONOMY_DEFS.items():\n",
        "        # Count how many unique labels were actually used in the dataset\n",
        "        used_categories_count = df_reconciled[axis].nunique()\n",
        "        total_categories = len(categories)\n",
        "        coverage = (used_categories_count / total_categories) * 100 if total_categories > 0 else 0\n",
        "        coverage_metrics[axis] = coverage\n",
        "    return coverage_metrics\n",
        "\n",
        "coverage_metrics = calculate_coverage(df_reconciled)\n",
        "\n",
        "def calculate_normalized_entropy(series):\n",
        "    \"\"\"Calculates the normalized entropy H(X) (bits) for a given series.\"\"\"\n",
        "    data = series.dropna()\n",
        "\n",
        "    if data.empty:\n",
        "        return 0.0\n",
        "\n",
        "    counts = data.value_counts()\n",
        "    probabilities = counts / len(data)\n",
        "    # Calculate entropy in bits (log base 2)\n",
        "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "\n",
        "    # Normalize by the maximum possible entropy (log2(N_categories))\n",
        "    n_categories = len(TAXONOMY_DEFS[series.name])\n",
        "    max_entropy = math.log2(n_categories) if n_categories > 1 else 0\n",
        "    normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
        "    return normalized_entropy\n",
        "\n",
        "# Calculate Entropy for Axis M\n",
        "entropy_M = calculate_normalized_entropy(df_reconciled['M'])\n",
        "\n",
        "def find_underexplored_pairs(df_reconciled, axis1='M', axis2='E', threshold=2):\n",
        "    \"\"\"Finds pairs of categories between two axes with counts below the threshold, including 0 counts.\"\"\"\n",
        "    df_clean = df_reconciled[[axis1, axis2]].dropna()\n",
        "\n",
        "    # Create a cross-tabulation (heatmap matrix)\n",
        "    crosstab = pd.crosstab(df_clean[axis1], df_clean[axis2])\n",
        "\n",
        "    # Ensure all defined categories from the taxonomy are represented by reindexing, filling missing with 0\n",
        "    try:\n",
        "        crosstab = crosstab.reindex(index=TAXONOMY_DEFS[axis1], columns=TAXONOMY_DEFS[axis2], fill_value=0)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Issue during reindexing MxE pairs: {e}.\")\n",
        "        return []\n",
        "\n",
        "    underexplored = []\n",
        "    for idx in crosstab.index:\n",
        "        for col in crosstab.columns:\n",
        "            # Ensure we only report on defined taxonomy labels\n",
        "            if idx in TAXONOMY_DEFS[axis1] and col in TAXONOMY_DEFS[axis2]:\n",
        "                count = crosstab.loc[idx, col]\n",
        "                if count < threshold:\n",
        "                    underexplored.append(f\"({idx} × {col}): {count}\")\n",
        "    return underexplored\n",
        "\n",
        "underexplored_ME = find_underexplored_pairs(df_reconciled, 'M', 'E', threshold=2)\n",
        "\n",
        "\n",
        "\n",
        "# Define output filenames\n",
        "RECONCILED_CSV_PATH = OUTPUT_DIR / \"taxonomy_labels_reconciled.csv\"\n",
        "RECONCILED_JSONL_PATH = OUTPUT_DIR / \"classification.jsonl\"\n",
        "METRICS_CSV_PATH = OUTPUT_DIR / \"taxonomy_metrics.csv\"\n",
        "FIG_M_PATH = OUTPUT_DIR / \"fig_counts_M.png\"\n",
        "FIG_A_PATH = OUTPUT_DIR / \"fig_counts_A.png\"\n",
        "README_PATH = OUTPUT_DIR / \"README.md\"\n",
        "ZIP_PATH = OUTPUT_DIR / \"taxonomy_validation_artifacts.zip\"\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Save Reconciled Data\n",
        "# For the CSV output, rename columns to include _rec suffix\n",
        "df_output_csv = df_reconciled.copy()\n",
        "df_output_csv.columns = [f'{col}_rec' if col in AXES else col for col in df_output_csv.columns]\n",
        "df_output_csv.to_csv(RECONCILED_CSV_PATH)\n",
        "\n",
        "# Save JSONL (using the standard axis names for content)\n",
        "df_reconciled.reset_index().to_json(RECONCILED_JSONL_PATH, orient='records', lines=True)\n",
        "\n",
        "# Save Metrics CSV\n",
        "metrics_data = []\n",
        "if not np.isnan(macro_kappa):\n",
        "    for axis in AXES:\n",
        "        metrics_data.append({\n",
        "            'Axis': axis,\n",
        "            'Metric': f'Cohen\\'s Kappa (κ)',\n",
        "            'Value': kappa_scores.get(axis, np.nan)\n",
        "        })\n",
        "        metrics_data.append({\n",
        "            'Axis': axis,\n",
        "            'Metric': f'Coverage (%)',\n",
        "            'Value': coverage_metrics.get(axis, np.nan)\n",
        "        })\n",
        "    metrics_data.append({'Axis': 'Overall', 'Metric': 'Macro-Average κ', 'Value': macro_kappa})\n",
        "\n",
        "metrics_data.append({'Axis': 'M', 'Metric': 'Normalized Entropy H_norm(M)', 'Value': entropy_M})\n",
        "metrics_data.append({'Axis': 'Overall', 'Metric': 'Data Coverage (%)', 'Value': overall_data_coverage})\n",
        "\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics_data)\n",
        "df_metrics.to_csv(METRICS_CSV_PATH, index=False)\n",
        "\n",
        "# Create README.md\n",
        "readme_content = f\"\"\"# Taxonomy Validation Artifacts (Reproduced)\n",
        "\n",
        "This directory contains the outputs of the taxonomy validation pipeline run on {N_papers} papers.\n",
        "\n",
        "## Key Metrics Summary:\n",
        "- Macro-Average Kappa: {macro_kappa:.4f}\n",
        "- Normalized Entropy H(M): {entropy_M:.4f}\n",
        "- Data Coverage (All axes coded): {overall_data_coverage:.2f}%\n",
        "\"\"\"\n",
        "with open(README_PATH, 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "# Zip Artifacts\n",
        "artifacts = [\n",
        "    RECONCILED_CSV_PATH, RECONCILED_JSONL_PATH, METRICS_CSV_PATH,\n",
        "    FIG_M_PATH, FIG_A_PATH, README_PATH\n",
        "]\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(ZIP_PATH, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "        for artifact in artifacts:\n",
        "            if artifact.exists():\n",
        "                # Write file using just the filename (arcname)\n",
        "                zf.write(artifact, arcname=artifact.name)\n",
        "    print(f\"\\nSuccessfully created ZIP archive: {ZIP_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: Failed to create ZIP file: {e}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "# 7. Reporting (Console Summary and LaTeX Output)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Console Summary\n",
        "print(\"=\"*60)\n",
        "print(\"Taxonomy Validation Pipeline Reproduced\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Processed {N_papers} papers. Data Coverage: {overall_data_coverage:.2f}%.\")\n",
        "\n",
        "if not np.isnan(macro_kappa):\n",
        "    print(\"\\n--- Inter-Rater Reliability (Cohen's Kappa) ---\")\n",
        "    for axis, kappa in kappa_scores.items():\n",
        "        print(f\"Axis {axis}: {kappa:.4f}\")\n",
        "    print(f\"Macro-Average Kappa: {macro_kappa:.4f}\")\n",
        "\n",
        "print(\"\\n--- Taxonomy Coverage Metrics (%) ---\")\n",
        "for axis, coverage in coverage_metrics.items():\n",
        "    print(f\"Axis {axis}: {coverage:.2f}%\")\n",
        "\n",
        "print(\"\\n--- Diversity Metrics ---\")\n",
        "print(f\"Normalized Entropy H(M): {entropy_M:.4f} bits\")\n",
        "\n",
        "print(\"\\n--- Under-explored (M×E) Pairs (Count < 2) ---\")\n",
        "count_displayed = 0\n",
        "for pair in underexplored_ME:\n",
        "     print(f\"- {pair}\")\n",
        "     count_displayed += 1\n",
        "     if count_displayed >= 10:\n",
        "         break\n",
        "if len(underexplored_ME) > 10:\n",
        "    print(f\"... and {len(underexplored_ME) - 10} more.\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "\n",
        "# LaTeX Output Generation\n",
        "def generate_latex_output(N_papers, kappa_scores, macro_kappa, coverage_metrics, entropy_M):\n",
        "    \"\"\"Generates the IEEE-tone LaTeX paragraph and table.\"\"\"\n",
        "\n",
        "    # Find axes with perfect agreement\n",
        "    perfect_agreement_axes = [axis for axis, k in kappa_scores.items() if k == 1.0]\n",
        "\n",
        "    # Specific formatting for the known results (E, D, L)\n",
        "    if set(perfect_agreement_axes) == {'E', 'D', 'L'}:\n",
        "        perfect_agreement_str = \"Environment (E), Data Type (D), and Learning Regime (L)\"\n",
        "    else:\n",
        "        # Fallback generic formatting\n",
        "        if len(perfect_agreement_axes) > 1:\n",
        "            perfect_agreement_str = \", \".join(perfect_agreement_axes[:-1]) + f\", and {perfect_agreement_axes[-1]}\"\n",
        "        elif len(perfect_agreement_axes) == 1:\n",
        "            perfect_agreement_str = perfect_agreement_axes[0]\n",
        "        else:\n",
        "            perfect_agreement_str = None\n",
        "\n",
        "    # High coverage axes\n",
        "    coverage_L = coverage_metrics['L']\n",
        "    coverage_M = coverage_metrics['M']\n",
        "\n",
        "    # Use double braces {{norm}} for correct f-string formatting of H_norm(M)\n",
        "    latex_paragraph = f\"\"\"\n",
        "\\\\subsection{{Taxonomy Validation and Reliability}}\n",
        "To validate the proposed taxonomy and ensure its consistent application, a dual-coder analysis was conducted on the corpus of {N_papers} papers (N={N_papers}). Two independent coders classified each paper across the six taxonomic axes. Inter-rater reliability was quantified using Cohen's Kappa ($\\\\kappa$), and the taxonomy's scope was evaluated via coverage percentage and normalized entropy H(M). The results, detailed in Table~\\\\ref{{tab:taxonomy_metrics_reproducible}}, demonstrate excellent overall reliability with a macro-averaged $\\\\kappa$ of {macro_kappa:.4f}. \"\"\"\n",
        "\n",
        "    if perfect_agreement_str:\n",
        "        latex_paragraph += f\"Perfect agreement ($\\\\kappa=1.0000$) was achieved for {perfect_agreement_str}, indicating high clarity and mutual exclusivity in these definitions. \"\n",
        "\n",
        "    # Specific wording based on the actual results (M=0.9338, A=0.8556, T=0.5053)\n",
        "    latex_paragraph += f\"Excellent agreement was observed for Model Type (M; $\\\\kappa={kappa_scores['M']:.4f}$) and Application Role (A; $\\\\kappa={kappa_scores['A']:.4f}$). The Threat Stage (T) axis exhibited moderate agreement ($\\\\kappa={kappa_scores['T']:.4f}$), primarily due to nuances in interpreting operational stages (e.g., INFER vs. MDR vs. ADAPT) from limited text. The taxonomy demonstrated broad coverage, utilizing {coverage_L:.2f}\\\\% of Learning Regimes and {coverage_M:.2f}\\\\% of Model Types. The normalized entropy for Axis M ($H_{{norm}}(M)={entropy_M:.4f}$) indicates a diverse utilization of generative models within the analyzed literature.\"\"\"\n",
        "\n",
        "    # Use double braces {{norm}} for correct f-string formatting of H_norm(M)\n",
        "    latex_table = f\"\"\"\n",
        "\\\\begin{{table}}[h]\n",
        "\\\\centering\n",
        "\\\\caption{{Taxonomy Reliability (Cohen's Kappa) and Coverage Metrics}}\n",
        "\\\\label{{tab:taxonomy_metrics_reproducible}}\n",
        "\\\\begin{{tabular}}{{lcc}}\n",
        "\\\\hline\n",
        "\\\\textbf{{Axis}} & \\\\textbf{{Cohen's Kappa ($\\\\kappa$)}} & \\\\textbf{{Coverage (\\\\%)}} \\\\\\\\\n",
        "\\\\hline\n",
        "M (Model Type) & {kappa_scores['M']:.4f} & {coverage_metrics['M']:.2f} \\\\\\\\\n",
        "A (Application Role) & {kappa_scores['A']:.4f} & {coverage_metrics['A']:.2f} \\\\\\\\\n",
        "E (Environment) & {kappa_scores['E']:.4f} & {coverage_metrics['E']:.2f} \\\\\\\\\n",
        "D (Data Type) & {kappa_scores['D']:.4f} & {coverage_metrics['D']:.2f} \\\\\\\\\n",
        "L (Learning Regime) & {kappa_scores['L']:.4f} & {coverage_metrics['L']:.2f} \\\\\\\\\n",
        "T (Threat Stage) & {kappa_scores['T']:.4f} & {coverage_metrics['T']:.2f} \\\\\\\\\n",
        "\\\\hline\n",
        "\\\\textbf{{Macro-Average $\\\\kappa$}} & \\\\multicolumn{{2}}{{c}}{{\\\\textbf{{{macro_kappa:.4f}}}}} \\\\\\\\\n",
        "\\\\textbf{{Normalized Entropy $H_{{norm}}(M)$}} & \\\\multicolumn{{2}}{{c}}{{\\\\textbf{{{entropy_M:.4f}}}}} \\\\\\\\\n",
        "\\\\hline\n",
        "\\\\end{{tabular}}\n",
        "\\\\end{{table}}\n",
        "\"\"\"\n",
        "    return textwrap.dedent(latex_paragraph), textwrap.dedent(latex_table)\n",
        "\n",
        "latex_paragraph, latex_table = generate_latex_output(N_papers, kappa_scores, macro_kappa, coverage_metrics, entropy_M)\n",
        "\n",
        "# Final Output Display\n",
        "print(\"\\n--- LaTeX Output (Ready for Copy/Paste) ---\")\n",
        "print(latex_paragraph)\n",
        "print(latex_table)\n",
        "print(\"--- End LaTeX Output ---\")"
      ]
    }
  ]
}